{"componentChunkName":"component---src-templates-category-template-js","path":"/posts/DL/","result":{"pageContext":{"currentCategory":"DL","categories":["All","iOS","웹개발","네트워크","음악","DL","R","자동화","PASAT","공부","한글"],"edges":[{"node":{"id":"ce357197-34e0-5a69-89fe-950c54405f18","excerpt":"참고링크: https://wikidocs.net/60680 활성화 함수 활성화 함수(activation function)이란 입력 신호의 총합을 출력 신호로 변환하는 함수를 의미한다. 인공 신경망의 활성화 함수는 비선형 함수를 사용해야 한다. 시그모이드(sigmoid) ReLU (Rectified Linear Unit)   단층 퍼셉트론 는 PyTorch에서 사용되는 선형 변환(linear transformation)을 수행하는 클래스이다. 두 개의 행렬 가중치(weight)와 편향(bias)을 학습하며, 입력 텐서를 선형 변환하여 출력 텐서를 생성한다. 선형 변환은 입력 텐서와 가중치 행렬의 행렬 곱을 계산하고, 편향을 더하는 연산으로 이루어진다. 로 입력값 weight을 모두 1로 초기화 로 linear layer 생성 로 활성화 함수 생성 입력값을 linear layer에 넣어서 output 생성 output을 ReLU에 넣어서 결과 출력  클래스를 사용하지 않고 수동으로 해…","fields":{"slug":"/deep-learning-1/"},"frontmatter":{"categories":"DL","title":"인공신경망","date":"September 15, 2023"}},"next":{"fields":{"slug":"/biostatistics-1/"}},"previous":{"fields":{"slug":"/music-history/"}}}]}},"staticQueryHashes":["1073350324","2009502679","2938748437"],"slicesMap":{}}